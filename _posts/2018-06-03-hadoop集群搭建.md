
---
layout: post
title: hadoop搭建
categories: 
    - 大数据
    - hadoop
description: 大数据
keywords: 大数据
comments: true
---

> hadoop单机版和集群搭建

#### 环境介绍：
系统： centos7　　
java: jdk-8u181-linux-x64 　
软件包存放：/data/　　
解压目录：/opt/　　
hadoop: hadoop-2.6.5.tar.gz　　
hive: apache-hive-2.3.4-bin.tar.gz　　
服务器的防火墙要关闭，并禁止开机自启

#### 系统java环境变量配置：

1. 解压java压缩包  

```python
tar -zxvf jdk-8u181-linux-x64.tar.gz -C /opt/
```
2. 配置环境变量  

```python
vi /etc/profile
export JAVA_HOME=/opt/jdk1.8.0_181
export JRE_HOME=${JAVA_HOME}/jre
export CLASSPATH=.:${JAVA_HOME}/lib:${JRE_HOME}/lib
export PATH=${JAVA_HOME}/bin:$PATH
source /etc/profile
```
#### 搭建hadoop
1. 解压hadoop压缩包  

```python
tar -zxvf hadoop-2.6.5.tar.gz -C /opt/
```
2. 配置hadoop环境变量

```python
vi /etc/profile
export HADOOP_HOME=/opt/hadoop-2.6.5
export PATH=$PATH:$HADOOP_HOME/bin
source /etc/profile
```
3. 配置hadoop   
    3.1 配置hadoop-env.sh  
    
    ```python
    vi /opt/hadoop-2.6.5/etc/hadoop/hadoop-env.sh
    # The java implementation to use.
    export JAVA_HOME=${JAVA_HOME}
    ```
    替换后  
    
    ```python
    export JAVA_HOME=/opt/jdk1.8.0_181
    
    ```
    配置yarn-env.sh
    ```python
    export JAVA_HOME=/opt/jdk1.8.0_181
    ```
    
    3.2 配置core-site.xml;file是hadoop路径，hdfs:是虚拟机ip
    
    ```python
    vi /opt/hadoop-2.6.5/etc/hadoop/core-site.xml
 
    <configuration>
       <property>
            <name>hadoop.tmp.dir</name>
            <value>/opt/hadoop-2.6.5/tmp</value>
            <description>Abase for other temporary directories.</description>
        </property>
        <property>
            <name>fs.defaultFS</name>
            <value>hdfs://172.16.161.150:8888</value>
        </property>     
         <property>
            <name>hadoop.proxyuser.root.hosts</name>
            <value>*</value>
         </property>
         <property>
            <name>hadoop.proxyuser.root.groups</name>
            <value>*</value>
         </property> 
           
    </configuration>

    ```
    3.3 配置hdfs-site.xml
    ```python
    vi /opt/hadoop-2.6.5/etc/hadoop/hdfs-site.xml
     <configuration>
      <property>
            <name>dfs.replication</name>
            <value>1</value>
        </property>
        <property>
            <name>dfs.namenode.name.dir</name>
            <value>/opt/hadoop-2.6.5/tmp/dfs/name</value>
        </property>
        <property>
            <name>dfs.datanode.data.dir</name>
            <value>/opt/hadoop-2.6.5/tmp/dfs/data</value>
        </property>

    </configuration>

    ```
4. 配置ssh免密登录　　

```python
ssh-keygen -t rsa 虚拟机没有.ssh目录
#本机的id_rsa.pub,复制到虚拟机
ssh-copy-id -i ~/.ssh/id_rsa.pub root@172.16.161.150
cat /root/.ssh/id_rsa.pub >> /root/.ssh/authorized_keys
```

5. Hadoop启动和停止

    5.1 首次需要初始化
    ```python
    cd /opt/hadoop-2.6.5
    ./bin/hdfs namenode -format 
    ```
    
    5.2 启动
    
    ```python
    ./sbin/start-dfs.sh
    ```
    ![](/images/hadoop/1.png)
    
    浏览器输入：http://172.16.161.150:50070
    
    ![](/images/hadoop/2.png)
    
    命令行输入：hadoop fs -mkdir /test
    
    在浏览器中能看到，刚创建的文件夹，就表示成功
    ![](/images/hadoop/３.png)
    
    5.3 停止
    ```python
    ./sbin/stop-dfs.sh
    ```

6. 配置yarn
    
    6.1 配置mapred-site.xml
    ```python
    cd /opt/hadoop-2.6.5/etc/hadoop/
    cp mapred-site.xml.template mapred-site.xml
    vi mapred-site.xml

    <configuration>
    <!-- 通知框架MR使用YARN -->
        <property>
            <name>mapreduce.framework.name</name>
            <value>yarn</value>
        </property>
    </configuration>

    ```
    6.2 配置yarn-site.xml
    
    ```python
    vi yarn-site.xml
    
    <configuration>
    <!-- Site specific YARN configuration properties -->
    
     <!-- reducer取数据的方式是mapreduce_shuffle -->
        <property>
            <name>yarn.nodemanager.aux-services</name>
            <value>mapreduce_shuffle</value>
        </property>
    </configuration>

    ```
    6.3 yarn启动与停止
    ```python
    cd /opt/hadoop-2.6.5
    ./sbin/start-yarn.sh  
    ```
    ![](/images/hadoop/4.png)
    浏览器输入：http://172.16.161.150:8088
    
    ![](/images/hadoop/5.png)
    
    停止　
    ```python
    ./sbin/stop-yarn.sh 
    ```
7. 查看进程
```python

[root@localhost hadoop-2.6.5]# jps
2032 SecondaryNameNode
1896 DataNode
1787 NameNode
2365 ResourceManager
2749 Jps
2638 NodeManager
[root@localhost hadoop-2.6.5]# 

```       

> 单机hadoop部署完毕

#### 安装hive

1. 解压hive压缩包
```python
tar -zxvf apache-hive-2.3.4-bin.tar.gz -C /opt/
```
配置hive环境变量
```python
export HIVE_HOME=/opt/apache-hive-2.3.4-bin
export PATH=$PATH:$HIVE_HOME/bin
```

2. 配置hive 
    2.1 创建HDFS目录
       
    ```python
    在 Hive 中创建表之前需要创建以下 HDFS 目录并给它们赋相应的权限。
    
    hdfs dfs -mkdir -p /user/hive/warehouse  
    hdfs dfs -mkdir -p /tmp/hive/  
    hdfs dfs -chmod 777 /user/hive/warehouse  
    hdfs dfs -chmod 777 /tmp/hive 
    #测试是否成功
    $HADOOP_HOME/bin/hadoop   fs   -ls   /user/hive/ 
    
    mkdri /opt/apache-hive-2.3.4-bin/tmp
    chmod 777 mkdri /opt/apache-hive-2.3.4-bin/tmp
    ```
    
    2.2 hive-site.xml
    ```python
    cd /opt/apache-hive-2.3.4-bin/conf
    
    cp hive-default.xml.template hive-site.xml
    
    vi hive-site.xml
    
    <?xml version="1.0" encoding="UTF-8" standalone="no"?>
    <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
    <configuration>
         <property>
            <name>javax.jdo.option.ConnectionURL</name>
            <value>jdbc:mysql://172.16.161.1:3306/hive?&amp;createDatabaseIfNotExist=true&amp;characterEncoding=UTF-8&amp;useSSL=false</value>
         </property>
        <property>
            <name>javax.jdo.option.ConnectionUserName</name>
            <value>hive</value>
        </property>
        <property>
            <name>javax.jdo.option.ConnectionPassword</name>
            <value>yuan121423</value>
        </property>
        <property>
            <name>javax.jdo.option.ConnectionDriverName</name>
            <value>com.mysql.jdbc.Driver</value>
        </property>
        <property>
            <name>datanucleus.schema.autoCreateAll</name>
            <value>true</value> </property>
        <property>
            <name>hive.metastore.schema.verification</name>
            <value>false</value>
         </property>
        <property>
            <name>hive.server2.authentication</name>
            <value>NOSASL</value>
            <description>
              Expects one of [nosasl, none, ldap, kerberos, pam, custom].
              Client authentication types.
                NONE: no authentication check
                LDAP: LDAP/AD based authentication
                KERBEROS: Kerberos/GSSAPI authentication
                CUSTOM: Custom authentication provider
                        (Use with property hive.server2.custom.authentication.class)
                PAM: Pluggable authentication module
                NOSASL:  Raw transport
            </description>
          </property>
       </configuration>
       #  将hive-site.xml文件中的${system:java.io.tmpdir}替换为hive的临时目录
       
    ```

    2.3 hive-env.sh
    ```python
    cp hive-env.sh.template hive-env.sh
    vi hive-env.sh
    
    # Set HADOOP_HOME to point to a specific hadoop install directory
    # HADOOP_HOME=${bin}/../../hadoop
    
    # Hive Configuration Directory can be controlled by:
    # export HIVE_CONF_DIR=
 
    ```
    修改成  
    
    ```python
    
        HADOOP_HOME=/opt/hadoop-2.6.5
        export HIVE_CONF_DIR=/opt/apache-hive-2.3.4-bin/conf

    ```
   2.4 初始化
   ```python
   schematool -initSchema -dbType mysql
    
    [root@localhost conf]# schematool -initSchema -dbType mysql 
    SLF4J: Class path contains multiple SLF4J bindings.
    SLF4J: Found binding in [jar:file:/opt/apache-hive-2.3.4-bin/lib/log4j-slf4j-impl-2.6.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]
    SLF4J: Found binding in [jar:file:/opt/hadoop-2.6.5/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
    SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
    SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
    Metastore connection URL:	 jdbc:mysql://172.16.161.1:3306/hive?createDatabaseIfNotExist=true
    Metastore Connection Driver :	 com.mysql.jdbc.Driver
    Metastore connection User:	 hive
    Loading class `com.mysql.jdbc.Driver'. This is deprecated. The new driver class is `com.mysql.cj.jdbc.Driver'. The driver is automatically registered via the SPI and manual loading of the driver class is generally unnecessary.
    Starting metastore schema initialization to 2.3.0
    Initialization script hive-schema-2.3.0.mysql.sql
    Initialization script completed
    schemaTool completed

  
   ```
3. 启动
```python
nohup hive --service metastore & 
nohup  hive --service hiveserver2 &
```


#### 报错

```python
Error: Could not open client transport with JDBC Uri: jdbc:hive2://172.16.161.150:10000: Failed to open new session: java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.authorize.AuthorizationException): User: root is not allowed to impersonate root (state=08S01,code=0)
```
解决：
```python
  <property>
	 <name>hadoop.proxyuser.root.hosts</name>
 	<value>*</value>
   </property>
   <property>
 	<name>hadoop.proxyuser.root.groups</name>
	 <value>*</value>
   </property>

```