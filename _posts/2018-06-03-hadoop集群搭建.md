
---
layout: post
title: hadoop搭建
categories: 
    - 大数据
    - hadoop
description: 大数据
keywords: 大数据
comments: true
---

> hadoop单机版和集群搭建

#### 环境介绍：
系统： centos7　　
java: jdk-8u181-linux-x64 　
软件包存放：/data/　　
解压目录：/opt/　　
hadoop: hadoop-2.6.5.tar.gz　　
hive: apache-hive-2.3.4-bin.tar.gz　　
服务器的防火墙要关闭，并禁止开机自启

#### 系统java环境变量配置：

1. 解压java压缩包  

```python
tar -zxvf jdk-8u181-linux-x64.tar.gz -C /opt/
```
2. 配置环境变量  

```python
vi /etc/profile
export JAVA_HOME=/opt/jdk1.8.0_181
export JRE_HOME=${JAVA_HOME}/jre
export CLASSPATH=.:${JAVA_HOME}/lib:${JRE_HOME}/lib
export PATH=${JAVA_HOME}/bin:$PATH
source /etc/profile
```
#### 搭建hadoop
1. 解压hadoop压缩包  

```python
tar -zxvf hadoop-2.6.5.tar.gz -C /opt/
```
2. 配置hadoop环境变量

```python
vi /etc/profile
export HADOOP_HOME=/opt/hadoop-2.6.5
export PATH=$PATH:$HADOOP_HOME/bin
source /etc/profile
```
3. 配置hadoop   
    3.1 配置hadoop-env.sh  
    
    ```python
    vi /opt/hadoop-2.6.5/etc/hadoop/hadoop-env.sh
    # The java implementation to use.
    export JAVA_HOME=${JAVA_HOME}
    ```
    替换后  
    
    ```python
    export JAVA_HOME=/opt/jdk1.8.0_181
    
    ```
    3.2 配置core-site.xml;file是hadoop路径，hdfs:是虚拟机ip
    
    ```python
    vi /opt/hadoop-2.6.5/etc/hadoop/core-site.xml
 
    <configuration>
        <property>
            <name>hadoop.tmp.dir</name>
            <value>file:///opt/hadoop-2.6.5</value>
            <description>Abase for other temporary directories.</description>
        </property>
        <property>
            <name>fs.defaultFS</name>
            <value>hdfs://172.16.161.150:8888</value>
        </property>
    </configuration>

    ```
    3.3 配置hdfs-site.xml
    ```python
    vi /opt/hadoop-2.6.5/etc/hadoop/hdfs-site.xml
     <configuration>
       <property>
            <name>dfs.replication</name>
            <value>1</value>
        </property>
        <property>
            <name>dfs.namenode.name.dir</name>
            <value>file:///opt/hadoop-2.6.5/tmp/dfs/name</value>
        </property>
        <property>
            <name>dfs.datanode.data.dir</name>
            <value>file:///opt/hadoop-2.6.5/tmp/dfs/data</value>
        </property>
    </configuration>

    ```
4. 配置ssh免密登录　　

```python
ssh-keygen -t rsa 虚拟机没有.ssh目录
#本机的id_rsa.pub,复制到虚拟机
ssh-copy-id -i ~/.ssh/id_rsa.pub root@172.16.161.150:~/.ssh/authorized_keys
```

5. Hadoop启动和停止

    5.1 首次需要初始化
    ```python
    cd /opt/hadoop-2.6.5
    ./bin/hdfs namenode -format 
    ```
    
    5.2 启动
    
    ```python
    ./sbin/start-dfs.sh
    ```
    ![](/images/hadoop/1.png)
    
    浏览器输入：http://172.16.161.150:50070
    
    ![](/images/hadoop/2.png)
    
    命令行输入：hadoop fs -mkdir /test
    
    在浏览器中能看到，刚创建的文件夹，就表示成功
    ![](/images/hadoop/３.png)
    
    5.3 停止
    ```python
    ./sbin/stop-dfs.sh
    ```

6. 配置yarn
    
    6.1 配置mapred-site.xml
    ```python
    cd /opt/hadoop-2.6.5/etc/hadoop/
    cp mapred-site.xml.template mapred-site.xml
    vi mapred-site.xml

    <configuration>
    <!-- 通知框架MR使用YARN -->
        <property>
            <name>mapreduce.framework.name</name>
            <value>yarn</value>
        </property>
    </configuration>

    ```
    6.2 配置yarn-site.xml
    
    ```python
    vi yarn-site.xml
    
    <configuration>
    <!-- Site specific YARN configuration properties -->
    
     <!-- reducer取数据的方式是mapreduce_shuffle -->
        <property>
            <name>yarn.nodemanager.aux-services</name>
            <value>mapreduce_shuffle</value>
        </property>
    </configuration>

    ```
    6.3 yarn启动与停止
    ```python
    cd /opt/hadoop-2.6.5
    ./sbin/start-yarn.sh  
    ```
    ![](/images/hadoop/4.png)
    浏览器输入：http://172.16.161.150:8088
    
    ![](/images/hadoop/5.png)
    
    停止　
    ```python
    ./sbin/stop-yarn.sh 
    ```
7. 查看进程
```python

[root@localhost hadoop-2.6.5]# jps
2032 SecondaryNameNode
1896 DataNode
1787 NameNode
2365 ResourceManager
2749 Jps
2638 NodeManager
[root@localhost hadoop-2.6.5]# 

```       

> 单机hadoop部署完毕