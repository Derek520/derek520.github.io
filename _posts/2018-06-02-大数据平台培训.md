---
layout: post
title: DMP架构
categories: 大数据
description: 大数据
keywords: 大数据
comments: true
---



### 爬虫：

1.爬虫的数据一定是可爬取的主题数据
2.爬虫运行时间是白天10：05点到次日4点

    - 不管任务是否执行完毕，4点都会停止，第二天10点再次运行
3.爬虫用的数据源：

    - 主题拆分小表，简称主题小表，表名：dmp_v5.DMP_A_SUBJECT_(主题名称),

4.主题转换：对主题小表中的VALUES字段的值进行转换，

    - 将原来小表中的values字段处理成data_result和match两个字段，会存储到主题转换表：spider.DMP_A_SUBJECT_(主题名称)_COV，原始表数据不变，新增两个字段data_result和match
    - data_result:是爬虫需要的id字段，
    - match:是需要匹配的结果字段
    - 脚本判断是如果是主题数据，就插入主题转化表中 spider.DMP_A_SUBJECT_（转换表）_COV）

5.生成待爬数据表

    - 需要主要转换表数据（spider.DMP_A_SUBJECT_（转换表）_COV）和90天爬虫结果累计表（spider.DMP_A_SUBJECT_（主题名称）_ALLINFO）
    - 转换表的中data_result字段是转换后的id,可以与主题id和平台id,内容内型，爬虫类型去重后使用，因为待爬虫是爬取近3天数据，也就是说待爬表中存储最近3天的数据
    - 要生成待爬数据表还需要关联2张表

        - 表1：90天爬虫结果累计表（spider.DMP_A_SUBJECT_（主题名称）_ALLINFO)
            - 如果90天的结果累计表有数据，说明我们爬取过，不需要再次爬取，关联后生成爬虫数据，和待爬数据
        - 表2:爬虫结果明细表（日）dmp_v4.DMP_A_SUBJECT_UNINFO_DTL（spider.DMP_A_SUBJECT_（主题名称）_INFO），这个表是每天爬虫结果
            - 取最近30天filter_code字段等于 ‘n’、状态码为“404”的数据，与表1关联后的待爬数数据中剔除
        - 将最终待爬数据，放进dmp_v4.DMP_A_SUBJECT_UNINFO表中


6.爬虫服务器拿到待爬数据后，将数据读到队列中，activeMQ数据库进行任务分发

    - 所以每天需要待爬的数据都会存储到MQ任务队列中

7.爬虫系统：

    - 分为爬虫服务器、爬虫下载器，爬虫解析器;分布系统是同步进行不是异步进行
    - 爬虫有个server端,负责爬虫开始
    -
8.MQ部署

    MQ部署在：192.168.1.34服务器 url=http://192.168.1.34:8161/admin/  admin/admin
    all_data        剩余待爬的数据
    dataBack        全部的数据  = all_data + successQueue
    resultBack      回传数据队列
    successQueue    成功数据队列

9.所有待爬的数据所在目录：

    spider/DMP_v5.0/yusp/server/profiles/r_n_(主题)/data

10.定时任务：  

    @Scheduled(cron="0 50 9 * * ?")                                # 每天9点50启动清除mq队列
    @Scheduled(cron=" 0 05 10 * * ?")                              # 每天10点5分启动爬虫服务
    50 9 * * * . /etc/profile;/bin/sh /usr/sh/start_resolve.sh     # 解析
    52 9 * * * . /etc/profile;/bin/sh /usr/sh/start_download_1.sh  # 下载
    53 9 * * * . /etc/profile;/bin/sh /usr/sh/start_download_2.sh  # 下载
    00 4 * * * . /etc/profile;/bin/sh /usr/sh/kill_java.sh         # 结束

11.执行文件目录：

    解析器执行目录：
    source ~/.bashrc
    cd /mnt/vm35/spider_resolve_1
    java -Xms7168m -Xmx7168m -jar spider_resolve_1.0.0.jar

    下载器执行目录：
    source ~/.bashrc
    cd /mnt/vm35/spider_download_1
    java -Xms7168m -Xmx7168m -jar spider_download_1.1.0.jar




爬虫代码执行流程：

    1.程序入口：launch.sh,创建一个进程执行bin\startup.sh
    2.bin\startup.sh：脚本判断java环境,若是正常，执行jar包:lib\cn.com.ultrapower.web.spider.App
    3.获取args：argsMap = ArgsUtils.parse(args);
    4.调用 SuperSpider.main(args);


    6.获取指定的时间：Calendar.getInstance();
    7.Thread.sleep(1000)暂停一秒 



脚本执行流程：

    1. 程序入口：deal.sh
        后台运行：/mnt/sdb/deal_flow/entrance/projectDeal_yun_main.sh  传4个参数
        # 执行projectDeal_yun_v5脚本,4个参数：jar转换程序，天数，路径，版本
        # $1:/mnt/sdb/deal_flow/convert/lib/Analysis_Convert_V4.0.1.6.jar  解析转换程序
        # $2:3天
        # $3:/mnt/sdc/yunftpuser
        # $4:v5.0.0
    2. projectDeal_yun_main.sh：
        判断（projectDeal_yun_v5.sh）上一批数据是否在运行和（updata_qiongju.sh.sh）穷举是否在运行
        运行则退出，没运行执行projectDeal_yun_v5.sh脚本，传4个参数：jar转换程序，天数，路径，版本
    3. projectDeal_yun_v5.sh：
         convert_yun.sh
            hadoop执行转换程序，生成log
         $1 : /mnt/sdb/deal_flow/convert/lib/Analysis_Convert_V4.0.1.6.jar
         $2 : 3			说明：处理几天前的数据
         $3 : /mnt/sdc/yunftpuser	说明：运公司数据目录, 格式如：/mnt/sdb/datang_data/yunftpuser/20180607
         $4 : v5.0.0			说明：版本
        这个脚本 创建Ua_list等目录，并分发到各个场景表，以及主题小表

    5. 遍历遍历/mnt/sdc/yunftpuser/日期/ 目录下的所有文件
    6. 每个文件执行/mnt/sdb/deal_flow/convert/convert_yun.sh 主题转换脚本，传6个参数
          # 执行脚本convert_yun.sh
          # 参数porv：所有key
          # 参数op：运营商
          # 参数dt：时间
          # 参数scn：场景表示
          # 参数codejar：/mnt/sdb/deal_flow/convert/lib/Analysis_Convert_V4.0.1.6.jar
          # 参数6：文件路径
    7.hadoop执行 Analysis_Convert_V4.0.1.6.jar，如果有“completed successfully”则执行，输出报告
    8.调用DmpScenehive.sh脚本生成就九大场景结果表，同时会诞生主题小表，如果是“subject”x 就是主题，登录爬虫服务器，
    9.登录192.1681.1.8服务器后，执行complement.sh，会诞生临时表，dmp_a_subject_video_cov_tmp
    10.固定时间调度，执行生成待爬数据




 



### 入库流程和九大场景

#### 主题模型

1. 主题模型是支持正则匹配，数据是按秒回传
2. 主题库不只是包含主题，内容包括行业主题、位置、融合ID、搜索词、身份证（19单独的）、临时主题（单独给个性化需求用的）6个。在流量解析后，会分成4个部分。
3. 分成主题、位置、融合id、搜索词

#### 专题模型

1. 专题模型支持模糊匹配，相同规则之跑一次，合并后回传
2.

#### 基础模型和app模型

1. 支持模糊，规则分优先级输出，合并
2. 此模型包含两部分：基础分类标签+APP标签
3. 任何一个url都会打上标签，有可能是app,也有可能不是app,是的话打上app标签




300 001 002 040


双主键：用户id和设备id

融合id:9001开始

此表是全部主题数据累计：dmp_v5.dmp_a_subject_manage 通过sub_id拆分

关联数据：以user_id,device_id,prov.op字段关联

行为id,如果是4为前2位是主题id,如果是3为前1位是主题id,(dmp_v4.DMP_C_DICT_SUB_ACTION

和app相关的都能映射到分类id  dmp_v4.DMP_C_DICT_SUB_PRODUCT)

运维平台：  

    1.需要配置客户id
    2.0-23 批次
    3.执行时间3点



